{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Regression Modeling\n",
    "## Due: Monday, Nov. 4th, 2024\n",
    "\n",
    "### Names: Keiran Berry\n",
    "### Course Level: Undergraduate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "* In this project, we explore building regression models to predict what you might expect for miles/gallon (MPG) fuel economy based on vehicle charateristics\n",
    "\n",
    "<u>**Note:** The project will be graded by me running your notebook from top to bottom (choosing the \"run all\" option) - if it errors out at any point - this is where I stop grading and you'll lose ALL points after the error - Even if they are correct!</u>\n",
    "\n",
    "* <u>Moral of the story is, **Make sure your entire notebook executes from top to bottom and you're happy with the results BEFORE you submit to the drop box!**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "* The objective of this project is to use scikit-learn to investigate different regression models for fuel economy estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's grab the data and have a look at the dataset\n",
    "\n",
    "## **Important note, to get the data, we need to install the UCIMLRepo package using pip**\n",
    "* In your command line, run the following:\n",
    "    - pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem A (60pts)**\n",
    "\n",
    "1. (5pts) Let's grab the data from UCI (you'll need to pip install ucimlrepo)\n",
    "\n",
    "* Note: a description of the dataset can be found [Here](https://archive.ics.uci.edu/dataset/9/auto+mpg)\n",
    "\n",
    "* <u><b>Questions:</b> How many features are there per observation? What do these features represent? How many classes? How many observations are in the data?</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo # Be sure to read above - need to install the ucimlrepo package using pip #\n",
    "  \n",
    "# fetch dataset \n",
    "# using import statements from dataset documentation page\n",
    "auto_mpg = fetch_ucirepo(id=9) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = auto_mpg.data.features \n",
    "y = auto_mpg.data.targets\n",
    "\n",
    "  \n",
    "# variable information, features, shapes, etc. \n",
    "print(\"Shape of data: \", X.shape)\n",
    "print(\"Shape of targets: \", y.shape)\n",
    "print(\"Observation information:\\n\", auto_mpg.variables) \n",
    "\n",
    "uniqueLabels = np.unique(y)\n",
    "print(\"\\nTotal classes: \", len(uniqueLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses:\n",
    "* There are 7 features per observation. This can be seen both in the shape of the data and in the printed observation information.\n",
    "* These features represent observable characteristics of the car whos mileage we are attempting to categorize, such as horsepower, weight, and model year.\n",
    "* There are 129 total classes, which can be seen in the number of unique labels found in our target matrix.\n",
    "* There are 398 observations in the dataset, which we can see in both shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the data and remove the observations with missing values #\n",
    "numXMissing = X.isnull().sum().sum() # use isnull to check for missing values, 2 sums for the 2d array \n",
    "numYMissing = y.isnull().sum().sum()\n",
    "print(\"Number of missing values in features:\", numXMissing)\n",
    "print(\"Number of missing values in target:\", numYMissing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Question: Is there missing data?  If so, which features are missing and how many observations are they missing in?</u>\n",
    "### Response:\n",
    "* Yes, there is missing data. We can see that when we use the dropna() function, some observations are dropped.\n",
    "* In the \"missing values\" printed area a couple cells above, we can see that feature 3 is the only one which has missing values. Referencing the table above that one, feature 3 appears to be horsepower.\n",
    "* This feature is missing in 6 of the observations, as we can see printed above. Removing these would change our total number of observations to 392."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (5pts.) Remove the missing data from both the feature set AND the associated targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important:  Get the index (rows) where the missing data exist in X BEFORE removing them! #\n",
    "missingIndices = X[X.isnull().any(axis=1)].index # getting the indices in case we need them later, but my method doesn't require them at the moment\n",
    "\n",
    "# Now, let's remove the rows (observations) with missing values and re-check the size of the data #\n",
    "# Don't forget to also drop the corresponding target values for each sample with missing data #\n",
    "cleanX = X.dropna()  # drop observations with missing features\n",
    "cleanY = y.loc[cleanX.index]  # drop target values that had their observations dropped\n",
    "\n",
    "# Print the new shapes for both Data (X) and targets (y)\n",
    "print(\"Shape of data after removing missing values:\", cleanX.shape)\n",
    "print(\"Shape of targets after removing missing values:\", cleanY.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, our data \"should\" be in good shape now - let's do some EDA\n",
    "\n",
    "3. (10 pts) Perform some EDA as outlined below and discuss your observations/results (i.e., provide a description of what the EDA is telling you about correlations, etc.)\n",
    "\n",
    "* Let's plot a scatterplotmatrix (should look similar to the one below): <u>What features do you notice are correlated, uncorrelated, outliers?, etc.  Describe what information you get from the plot below</u>\n",
    "\n",
    "<u>Describe your plot here:</u>\n",
    "### Response:\n",
    "* The features which I can see are correlated have a clear diagonal trend. For example, weight and horsepower, weight and displacement, and mpg and model year all have strong positive correlation. Some examples of features with negative correlation include acceleration and horsepower, mpg and weight, and mpg and horsepower. These correlations in relation to the targets also make sense, as manufacturers are constantly trying to improve mpg, as well as more weight resulting in less mpg. These correlations line up with my preconceived notions about mileage.\n",
    "* Some examples of uncorrelated data, or data with no clear trend, include many of the model year statistics. Obviously, many different vehicles of different weights, horsepower, and cylinders will be made every year, so there is no clear trend.\n",
    "* Some outliers are visible in this scatterplotmatrix, such as a vehicle with a particularly high horsepower for a lower weight, and a particularly high horsepower for a specific mpg. These outliers may make some classifications difficult, as they do not follow the trend of features which are correlated, but as there is still a strong correlation those features are certainly important to classification.\n",
    "\n",
    "<img src=\"Figures/ScatterPlotMat.png\" alt=\"Scatter Plot Matrix\" style=\"width:500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Need to import mlextend\n",
    "# conda install conda-forge::mlxtend\n",
    "\n",
    "from mlxtend.plotting import scatterplotmatrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatterplotmatrix to get an idea about how correlated the features are across samples #\n",
    "# Note: we currently have the data features in X and the targets in y.  We need to concatinate them\n",
    "# so we can get an idea about correlations w.r.t. the targets too #\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.concat([cleanX, cleanY], axis=1)\n",
    "scatterplotmatrix(data.values, alpha=0.5, figsize=(20, 16), names=data.columns)\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, hspace=0.5, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's also look at a correlation plot (should look similar to the plot below): <u>What features do yo notice are correlated, uncorrelated, etc.  Describe what information you get from the plot below</u>\n",
    "\n",
    "<u>Describe your plot here:</u>\n",
    "### Response:\n",
    "* Looking at the correlation matrix, I can clearly see which features have strong correlations or lack thereof. I notice that mpg is strongly negatively correlated with displacement, cylinders, horsepower, and weight. A couple of positive correlation examples include displacement and cylinders, displacement and weight, and cylinders and weight.\n",
    "* The uncorrelated features are those closest to zero, for example origin and model year or acceleration and origin. These make sense as things that would not be correlated.\n",
    "* This plot reads much easier to me than the scatterplotmatrix, as it is easier to see a concrete number to compare one potential correlation to another. I can see where things I thought were more correlated, such as mpg and model year, are not as correlated as I thought when looking at objective numbers.\n",
    "<img src=\"Figures/CorrelationPlot.png\" alt=\"Correlation Plot\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.plotting import heatmap\n",
    "# Look at the correlation matrix #\n",
    "correlationMatrix = data.corr()\n",
    "heatmap(correlationMatrix.values, \n",
    "        row_names=data.columns, \n",
    "        column_names=data.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, let's go ahead and build a model:  **You can choose if you want to build a simple linear regression model OR a robust regression model (RANSAC)**\n",
    "\n",
    "##### **Note:** Don't forget, we need to: a) scale the data and b) do a test_train_split to evaluate model performance (let's do 80% train and 20% test this time)\n",
    "\n",
    "4. (40pts.) Build a linear regression model using a single feature of your choice (I chose weight but feel free to pick what you find interesting from the EDA above).  Perform a performance analysis of your second model using:\n",
    "    - Residues\n",
    "    - $R^2$ (coefficient of determination)\n",
    "\n",
    "* The first model you build will be a simple linear model of your_feature vs. mpg.\n",
    "* The second model will include ALL of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looked like weight was highly correlated with mpg so I grabbed this data (use whatever feature you like here) #\n",
    "horsepower = cleanX[['horsepower']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Scale the data using standardScaler #\n",
    "hpScaler = StandardScaler()\n",
    "yScaler = StandardScaler()\n",
    "scaledHorsepower = hpScaler.fit_transform(horsepower)\n",
    "scaledY = yScaler.fit_transform(cleanY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data 80/20 #\n",
    "trainHorsepower, testHorsepower, trainY, testY = train_test_split(scaledHorsepower, scaledY, test_size = 0.2, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot the regression line #\n",
    "def lin_regplot(X, y, model):\n",
    "    plt.scatter(X, y, c='steelblue', edgecolor='white', s=70)\n",
    "    plt.plot(X, model.predict(X), color='black', lw=2)    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a simple linear regression model on a single feature vs. mpg (be sure to plot the model coefficents and intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sklearn Regression model (not scaled)#\n",
    "linearRegression = LinearRegression()\n",
    "linearRegression.fit(trainHorsepower, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model against a scatter plot of the data\n",
    "\n",
    "**IF you chose weight for your feature, your plot should look something like this:**\n",
    "\n",
    "<img src=\"Figures/ScatterPlot1.png\" alt=\"Original Scatter\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the model and see how well it does #\n",
    "plt.figure(figsize=(10, 6))\n",
    "lin_regplot(trainHorsepower, trainY, linearRegression)\n",
    "\n",
    "coefficients = linearRegression.coef_\n",
    "intercept = linearRegression.intercept_\n",
    "print(\"Coefficient(s): \", coefficients)\n",
    "print(\"Intercept: \", intercept)\n",
    "\n",
    "plt.xlabel('Vehicle Horsepower')\n",
    "plt.ylabel('Miles per Gallon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Note:** We're still in scaled space, let's generate a plot just like the one above, but in the unscaled space so we can really see what's going on with the relationship between vehicle weight and MPG\n",
    "\n",
    "* You need to do the predicitons in scaled space, then invert BOTH the predictions and the test_features back to unscaled space\n",
    "\n",
    "**Your plot should look like this**\n",
    "\n",
    "<img src=\"Figures/ScatterPlot2.png\" alt=\"Normal Scatter\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the test set to see what the predicted mpg is #\n",
    "predictions = linearRegression.predict(testHorsepower)\n",
    "\n",
    "# Now we need to \"bring the prediction\" back from un-scaled space #\n",
    "# Also need to bring the test features back #\n",
    "predictions = yScaler.inverse_transform(predictions)\n",
    "testHorsepower = hpScaler.inverse_transform(testHorsepower)\n",
    "testY = yScaler.inverse_transform(testY)\n",
    "\n",
    "# Let's plot the model and see how well it does #\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(testHorsepower, testY, c='steelblue', edgecolor='white', s=70)\n",
    "plt.plot(testHorsepower, predictions, color='black', lw=2)\n",
    "plt.xlabel('Vehicle Horsepower')\n",
    "plt.ylabel('Miles per Gallon')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Okay, let's build a linear model using the entire dataset, and evaluate the performance\n",
    "\n",
    "1. First generate a residue plot for the full model (i.e., using all features given)\n",
    "2. Then look at the $R^2$ score for the model\n",
    "\n",
    "<u>Question:</u> How well does the model work? (Answered below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the full dataset (if yoyu didn't do this above and only scaled a single feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you didn't do this above, scale the entire dataset including all features #\n",
    "# targets have already been scaled, but not data\n",
    "xScaler = StandardScaler()\n",
    "scaledX = xScaler.fit_transform(cleanX)\n",
    "\n",
    "# Perform a new test/train split on the entire scaled dataset #\n",
    "# Split the data 80/20 #\n",
    "trainX, testX, trainY, testY = train_test_split(scaledX, scaledY, test_size = 0.2, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the full model and print the model parameters (weights and bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and perform the predictions on the test set #\n",
    "# Print the intercept and coefficients #\n",
    "linearRegression = LinearRegression() # just making sure we have a fresh one in case of unintended side effects\n",
    "linearRegression.fit(trainX, trainY)\n",
    "testPredictions = linearRegression.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the residues for both testing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the residues #\n",
    "# We have the predicted test set, let's predict the training set and plot the residues for both #\n",
    "trainPredictions = linearRegression.predict(trainX)\n",
    "\n",
    "# unscaling so that the values are real\n",
    "trainPredictions = yScaler.inverse_transform(trainPredictions)\n",
    "testPredictions = yScaler.inverse_transform(testPredictions)\n",
    "unscaledTrainY = yScaler.inverse_transform(trainY)\n",
    "unscaledTestY = yScaler.inverse_transform(testY)\n",
    "\n",
    "# Plot residuals\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "display = PredictionErrorDisplay(y_true=unscaledTrainY, y_pred=trainPredictions)\n",
    "display.plot()\n",
    "plt.title('Training Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "display = PredictionErrorDisplay(y_true=unscaledTestY, y_pred=testPredictions)\n",
    "display.plot()\n",
    "plt.title('Testing Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the $R^2$ value:\n",
    "\n",
    "<u>Question:</u> How well does the model perform? (discuss)\n",
    "### Response:\n",
    "* The model performs relatively well, with an $R^2$ value of about .82 on both the training and testing data. This means that the model explains about 82% of the variability in the target variable.\n",
    "* Looking at the residual plots, we can see that most of the predictions are generally within +/- 2.5 mpg of the target. This is overall a good result. One thing which the model seems to struggle with is picking up those few outliers with particularly high mpg. This can be seen in the higher predicted values section, where the model predicted 30-35 mpg and the residual value was still 10 or higher. Since this means that these vehicles had over 40-45 mpg, it makes sense that it would be hard to make a guess at these outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# Compute and display the R^2 for both training and testing sets #\n",
    "\n",
    "trainR2 = r2_score(unscaledTrainY, trainPredictions)\n",
    "testR2 = r2_score(unscaledTestY, testPredictions)\n",
    "\n",
    "print(\"Training R^2: \", trainR2)\n",
    "print(\"Testing R^2: \", testR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem B (40pts)**\n",
    "\n",
    "1. (30pts) Let's \"redo\" the above analysis but build a regression tree as opposed to a Linear model\n",
    "\n",
    "* Don't worry about building the \"single feature\" tree, let's just use the entire dataset and:\n",
    "    - Plot the residuals\n",
    "    - Compute the $R^2$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression tree (choos what depth you want - play with this to see how it effects model evaluation) #\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "decisionTree = DecisionTreeRegressor(random_state=1, max_depth=7)\n",
    "decisionTree.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "tree.plot_tree(decisionTree, filled=True)\n",
    "plt.title(\"Decision Tree Regressor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the prediction on the test set, and perform model evaluation as outlined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for all samples #\n",
    "testPredictions = decisionTree.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the residues #\n",
    "# We have the predicted test set, let's predict the training set #\n",
    "trainPredictions = decisionTree.predict(trainX)\n",
    "\n",
    "# unscaling so that the values are real\n",
    "trainPredictions = yScaler.inverse_transform(trainPredictions.reshape(-1, 1))\n",
    "testPredictions = yScaler.inverse_transform(testPredictions.reshape(-1, 1))\n",
    "unscaledTrainY = yScaler.inverse_transform(trainY)\n",
    "unscaledTestY = yScaler.inverse_transform(testY)\n",
    "\n",
    "# Plot residuals\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "display = PredictionErrorDisplay(y_true=unscaledTrainY, y_pred=trainPredictions)\n",
    "display.plot()\n",
    "plt.title('Training Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "display = PredictionErrorDisplay(y_true=unscaledTestY, y_pred=testPredictions)\n",
    "display.plot()\n",
    "plt.title('Testing Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the $R^2$ value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display the R^2 for both training and testing sets #\n",
    "trainR2 = r2_score(unscaledTrainY, trainPredictions)\n",
    "testR2 = r2_score(unscaledTestY, testPredictions)\n",
    "\n",
    "print(\"Training R^2: \", trainR2)\n",
    "print(\"Testing R^2: \", testR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "* This classifier does not do as well with generalizing as the linear regression model which we trained earlier. The $R^2$ value for the training set is generally between .95 and .99, at various depths. While these are high, the $R^2$ value for the testing set is only about .76 at a max depth of 7.\n",
    "* We can force the model to do slightly better, such as getting an $R^2$ of about .8 at depth of 10, which of course results in overtraining as our $R^2$ value on the training set gets very close to 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (10pts) Perform the exact same experiements as in problem B.1 but using a Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "randomForestRegressor = RandomForestRegressor(random_state=1, max_depth=5)\n",
    "randomForestRegressor.fit(trainX, trainY.ravel()) # have to flatten y into a 1d array\n",
    "\n",
    "# random forest regressor isn't super \"plottable\" unless we just want to look at a single tree\n",
    "# plotting one just for fun\n",
    "plt.figure(figsize=(12, 8))\n",
    "tree.plot_tree(randomForestRegressor.estimators_[0], filled=True)\n",
    "plt.title(\"First Tree in Random Forest Regressor\")\n",
    "plt.show()\n",
    "\n",
    "# Predict for all samples #\n",
    "testPredictions = randomForestRegressor.predict(testX)\n",
    "\n",
    "# Let's plot the residues #\n",
    "# We have the predicted test set, let's predict the training set #\n",
    "trainPredictions = randomForestRegressor.predict(trainX)\n",
    "\n",
    "# unscaling so that the values are real\n",
    "trainPredictions = yScaler.inverse_transform(trainPredictions.reshape(-1, 1))\n",
    "testPredictions = yScaler.inverse_transform(testPredictions.reshape(-1, 1))\n",
    "unscaledTrainY = yScaler.inverse_transform(trainY)\n",
    "unscaledTestY = yScaler.inverse_transform(testY)\n",
    "\n",
    "# Plot residuals\n",
    "\n",
    "display = PredictionErrorDisplay(y_true=unscaledTrainY, y_pred=trainPredictions)\n",
    "display.plot()\n",
    "plt.title('Training Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "display = PredictionErrorDisplay(y_true=unscaledTestY, y_pred=testPredictions)\n",
    "display.plot()\n",
    "plt.title('Testing Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "# Compute and display the R^2 for both training and testing sets #\n",
    "trainR2 = r2_score(unscaledTrainY, trainPredictions)\n",
    "testR2 = r2_score(unscaledTestY, testPredictions)\n",
    "\n",
    "print(\"Training R^2: \", trainR2)\n",
    "print(\"Testing R^2: \", testR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "* This classifier does much better overall than the Decision Tree Regressor. While the Decision Tree Regressor mainly had $R^2$ values in the .7 - .8 range, the Random Forest Regressor lands in the .85 range consistently.\n",
    "* This also makes the Random Forest Regressor competitive with the Linear Regression model, which was in the .82 range. With a number of depths, from 5 to 10, there is not much increase in $R^2$ value as depth increases. At a depth of 10, I was able to achieve an $R^2$ value of about .86, so it defititely makes more sense to remove complexity for similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSC 549 Students Only!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that some of the features illustrated in Problem A.3 (scatterplotmatrix) show that the correlation between that feature and the mpg is actually somewhat nonlinear\n",
    "\n",
    "* Build a nonlinear regression model (e.g. some polynomial function) using horsepower vs. mpg\n",
    "* Evaluate the residuals, $R^2$, and discuss your solution (e.g., what model ended up working well?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
